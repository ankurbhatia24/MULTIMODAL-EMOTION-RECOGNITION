This Directory hosts the links to the audio datasets,brief descriptions and how useful they are to our use case:-

- General Good resource: [OpenSLR Datasets](http://www.openslr.org/resources.php)
- Springer List of datasets with emotional speech: [Emotional Speech Databases](https://link.springer.com/content/pdf/bbm%3A978-90-481-3129-7%2F1.pdf) 
- Needs a account to explore the database:- [sensitive agent project database](https://semaine-db.eu/)
- Surrey Audio-Visual Expressed Emotion (SAVEE) Database - [Link](http://kahlan.eps.surrey.ac.uk/savee/Introduction.html)
- India Emotional Speech Dataset- [link](https://arxiv.org/pdf/1910.13801.pdf)
- CREMA-D - 7442 files, English, 6 emotions, speech, 2443 raters- [link](https://ieeexplore.ieee.org/abstract/document/6849440/)
- MSP-IMPROV - Dyadic speakers, English, 7818 files, 5 emotions, many raters.[link](http://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Improv.html)
- There are also a number of non-validated sets, some of which include: RML emotion database, SAVEE, eNTERFACE'05.

# Human Audio Datasets

## [Multimodal EmotionLines Dataset (MELD)](https://affective-meld.github.io/)

- Multimodal EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset.
- MELD contains the same dialogue instances available in EmotionLines, but it also encompasses audio and visual modality along with text. 
- MELD has more than 1400 dialogues and 13000 utterances from Friends TV series.
- Each utterance in a dialogue has been labeled with— Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. Download here

For this project:
- This will be the most important dataset for the project and I believe it has already been used in this research project.

## [IEMOCAP Datset](https://sail.usc.edu/iemocap/iemocap_release.htm):
- . It contains data from 10 actors, male and female, during their affective dyadic interaction. 
-   approximately 12 hours of audiovisual data
- For each improvised and scripted recording, we provide detailed audiovisual and text information, 
which consists of the audio and video of both interlocutors, the Motion Capture data of the face, head 
and hand of one of the interlocutors in each recording,
the text trascriptions of the conversation and their word-level, phone-level and syllable-level alignment.

For this project:
- It will also very important for learning about skills of audio analysis to identify emotions in dyadic interactions.

## [Toronto emotional speech set (TESS) ](https://tspace.library.utoronto.ca/handle/1807/24487)
- A set of 200 target words were spoken in the carrier phrase "Say the word _____' by two actresses (aged 26 and 64 years) and 
recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral).
- There are 2800 stimuli in total.

For this Project:-
- This will also be very important dataset as it can be used to do emotion classification using audio features

## [Ryerson Audio-Visual Database of Emotional Speech and Song](https://smartlaboratory.org/ravdess)
-contains 7356 files (total size: 24.8 GB). 
- The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. 
- Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. 

For this project:
-Among little few datasets that can be used for emotion classification directly after extracting audio features

## [CHIME](https://chimechallenge.github.io/chime6/download.html)
This is a noisy speech recognition challenge dataset (~4GB in size). 
The dataset contains real simulated and clean voice recordings.
Real being actual recordings of 4 speakers in nearly 9000 recordings over 4 noisy locations, 
simulated is generated by combining multiple environments over speech utterances and clean being non-noisy recordings.

For this project:-
- Not useful directly for emotion classfication(not sure) as such but will help in learn in reducing noise,identifying how noise affect the data and the audio features and analysis

## http://kahlan.eps.surrey.ac.uk/savee/
## [Free Spoken Digit Dataset](https://github.com/Jakobovski/free-spoken-digit-dataset)

- This one was created to solve the task of identifying spoken digits in audio samples.
- It’s an open dataset so the hope is that it will keep growing as people keep contributing more samples.
- Currently, it contains the below characteristics: 1) 3 speakers 2) 1,500 recordings (50 of each digit per speaker) 3) English pronunciations.
- This is a really small set- about 10 MB in size.

For this Project:-
- Not Much of Use to us,as we in general want to identify emotion from sentences that are spoken by speakers using NLP and using audio features analysis

## [LibriSpeech](http://www.openslr.org/12/)

- This dataset is a large-scale corpus of around 1000 hours of English speech.
- The data has been sourced from audio books from the LibriVox project and is 60 GB in size.

For this project:- 
- This can be useful for our project as hold reading by the speaker Vassil Panayotov with the assistance of Daniel Povey.

## [VoxCeleb](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/)

- VoxCeleb is a large-scale speaker identification dataset.
- It contains around 100,000 utterances by 1,251 celebrities, extracted from You Tube videos. 
- The data is mostly gender balanced (males comprise of 55%).
- The celebrities span a diverse range of accents, professions, and age.
- There is no overlap between the development and test sets. 
- It’s an intriguing use case for isolating and identifying which superstar the voice belongs to.
- This set is 150 MB in size and has about 2000 hours of speech.

For this Project:- 
 - This may be useful to our use case too,atleast for audio analysis skill learning.
 - It doesn't carry text data so how useful it is in reality is yet to be determined.
 
## [The Spoken Wikipedia Corpora](https://nats.gitlab.io/swc/)

- This is a corpus of aligned spoken Wikipedia articles from the English, German, and Dutch Wikipedia.
- Hundreds of hours of aligned audio and annotations can be mapped back to the original HTML.
- The entire set is about 38 GB in size available in both audio and without audio format.

For this project:- 
- It will prove to useful as contains both aligned audio and annotations,which can be used for use case


## [TIMIT Corpus](https://catalog.ldc.upenn.edu/LDC93S1)

- The TIMIT corpus (440 MB) of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition
systems. 
- TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. 
- It includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16 kHz speech waveform file for each utterance.

For this Project:-
- I think this will be useful in learning about audio anlaysis and not as such direct emotion classification tasks

## [Flickr Audio Caption Corpus](https://groups.csail.mit.edu/sls/downloads/flickraudio/)

- 40,000 spoken captions of 8,000 natural images, 4.2 GB in size.
- This corpus was collected in 2015 to investigate multi-modal learning schemes for unsupervised speech pattern discovery.

For this Project:
- I don't think it will be as useful for our use case here for our use case,but it can audio anlaysis learning process

## [TED-LIUM](https://www.openslr.org/7/)
- Audio transcription of TED talks. 
- 1495 TED talks audio recordings along with full-text transcriptions of those recordings,
created by Laboratoire d’Informatique de l’Université du Maine (LIUM).

For this Project:-
- I think this will be pretty useful for emotion identification as it has both audio recordings and full text transcriptions
.I understand that it doesn't explicitly identifies any emotions in dataset.But it can be useful for experimenting and audio analysis learning process..

## [Speech Commands Dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html)
- The dataset (1.4 GB) has 65,000 one-second long utterances of 30 short words, by thousands of different people,
contributed by members of the public through the AIY website.
- The dataset is designed to let you build basic but useful voice interfaces for applications, with common words like “Yes”, “No”, digits and directions included.
The infrastructure used to create the data has been open sourced too, and we hope to see it used by the wider community to create their own versions, 
especially to cover under served languages and applications.

For this project:
- I think it can be used,not our research project by identifying several audio features and deducing results about the person speaking them..

## [Common Voice](https://voice.mozilla.org/en/about)
Common Voice (12 GB is size) is a corpus of speech data read by users on the Common Voice website, and
based on text from a number of public domain sources like user-submitted blog posts, old books, movies, and other public speech corpora. 
Its primary purpose is to enable the training and testing of automatic speech recognition (ASR) systems.

## [VoxForge](http://www.voxforge.org/home/downloads)
Clean speech dataset of accented English. Useful for instances in which you expect to need robustness to different accents or intonations.

For this project:-
- Can be used to learn how accents affect the audio features and analysis components

## [2000 HUB5 English](https://catalog.ldc.upenn.edu/LDC2002T43)
English-only speech data used most recently in the Deep Speech paper from Baidu.
Parkinson Speech Dataset with Multiple Types of Sound Recordings Data Set
The training data belongs to 20 Parkinson’s Disease (PD) patients and 20 healthy subjects. 
From all subjects, multiple types of sound recordings (26) are taken for this 20 MB set.

For this project:-
- Not sure if it will be directly useful,but it can used to learn patterns in speech of people with diseases.

## [Zero Resource Speech Challenge](https://www.zerospeech.com/)
The ultimate goal of the Zero Resource Speech Challenge is to construct a system that learns an end-to-end Spoken Dialog (SD) system,
in an unknown language, from scratch, using only information available to a language learning infant.
“Zero resource” refers to zero linguistic expertise (e.g., orthographic/linguistic transcriptions),
not zero information besides audio (visual, limited human feedback, etc). 
The fact that 4-year-olds spontaneously learn a language without supervision from language experts show that this goal is theoretically reachable.

For this project:-
- Not sure where it can be used here in our use case right now but it can form basis for future work..
